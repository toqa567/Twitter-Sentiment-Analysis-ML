{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F96HRQN_Nu-M"
      },
      "source": [
        "install **libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r1Mvav3ODJR"
      },
      "outputs": [],
      "source": [
        "#Hagar Mahmoud Mohamed      420220824\n",
        "#Rama Mohamed Mutaz         420220953\n",
        "#Toqa Abselnasser           420220640\n",
        "#Rowaida Gamal Hassan       420221020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A10ESCdMYbYV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIXOMKirT2lN"
      },
      "source": [
        "download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMN3OJ2PTyWD",
        "outputId": "d2e82288-37d4-42c7-ce9a-ef247a5418dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /kaggle/input/sentiment140\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPOFB5M0N6ZM",
        "outputId": "667a4842-25c7-40f8-8cd0-4c0587bc1768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /kaggle/input/sentiment140\n",
            "Reading file: /kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\n",
            "   polarity                                               text\n",
            "0         0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
            "1         0  is upset that he can't update his Facebook by ...\n",
            "2         0  @Kenichan I dived many times for the ball. Man...\n",
            "3         0    my whole body feels itchy and like its on fire \n",
            "4         0  @nationwideclass no, it's not behaving at all....\n",
            "         polarity                                               text\n",
            "1599995         4  Just woke up. Having no school is the best fee...\n",
            "1599996         4  TheWDB.com - Very cool to hear old Walt interv...\n",
            "1599997         4  Are you ready for your MoJo Makeover? Ask me f...\n",
            "1599998         4  Happy 38th Birthday to my boo of alll time!!! ...\n",
            "1599999         4  happy #charitytuesday @theNSPCC @SparksCharity...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "\n",
        "zip_path = path\n",
        "extract_dir = \"/kaggle/input/sentiment140_extracted\"\n",
        "\n",
        "\n",
        "if zip_path.endswith(\".zip\") and not os.path.exists(extract_dir):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    data_dir = extract_dir\n",
        "else:\n",
        "    data_dir = path\n",
        "\n",
        "csv_file = None\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "    for f in files:\n",
        "        if f.lower().endswith(\".csv\"):\n",
        "            csv_file = os.path.join(root, f)\n",
        "            break\n",
        "    if csv_file:\n",
        "        break\n",
        "\n",
        "if csv_file is None:\n",
        "    raise FileNotFoundError(\"لم يتم العثور على ملف CSV داخل المسار الناتج.\")\n",
        "\n",
        "print(\"Reading file:\", csv_file)\n",
        "\n",
        "df = pd.read_csv(csv_file, encoding='latin-1', header=None)\n",
        "\n",
        "df = df[[0, 5]]\n",
        "df.columns = ['polarity', 'text']\n",
        "\n",
        "print(df.head())\n",
        "print(df.tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzOEuXX7VLSm"
      },
      "source": [
        "Keep Only Positive and Negative Sentiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKwICibaQxtk",
        "outputId": "cb914609-94c8-4b2c-d233-1c02a46d08e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "polarity\n",
            "0    800000\n",
            "1    800000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df = df[df.polarity != 2]\n",
        "\n",
        "df['polarity'] = df['polarity'].map({0: 0, 4: 1})\n",
        "\n",
        "print(df['polarity'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGcSI_nLVURW"
      },
      "source": [
        "Step 4: Clean the Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9pyen6kP81P",
        "outputId": "4770a349-dcab-4839-c158-71a027f9aa73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  \\\n",
            "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
            "1  is upset that he can't update his Facebook by ...   \n",
            "2  @Kenichan I dived many times for the ball. Man...   \n",
            "3    my whole body feels itchy and like its on fire    \n",
            "4  @nationwideclass no, it's not behaving at all....   \n",
            "\n",
            "                                          clean_text  \n",
            "0  @switchfoot http://twitpic.com/2y1zl - awww, t...  \n",
            "1  is upset that he can't update his facebook by ...  \n",
            "2  @kenichan i dived many times for the ball. man...  \n",
            "3    my whole body feels itchy and like its on fire   \n",
            "4  @nationwideclass no, it's not behaving at all....  \n"
          ]
        }
      ],
      "source": [
        "def clean_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "print(df[['text', 'clean_text']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFuBLiIaVcFG"
      },
      "source": [
        "Step 5: Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoIUjqDNRXFh",
        "outputId": "a65a3bf8-a9ea-49d3-b490-e0ba5efd2293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 1280000\n",
            "Test size: 320000\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['clean_text'],\n",
        "    df['polarity'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(X_train))\n",
        "print(\"Test size:\", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhJZNFoXVjXd"
      },
      "source": [
        "Step 6: Perform Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCLvrsFOVkx_",
        "outputId": "f2c74a8a-d94a-49e3-9553-18fd35fbacfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF shape (train): (1280000, 5000)\n",
            "TF-IDF shape (test): (320000, 5000)\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(\"TF-IDF shape (train):\", X_train_tfidf.shape)\n",
        "print(\"TF-IDF shape (test):\", X_test_tfidf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LDtPsX_V9v4"
      },
      "source": [
        "Step 7: Train Bernoulli Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdGbFpXlV-yv",
        "outputId": "f23417d0-bcee-4524-c0a0-9a142a7ced8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bernoulli Naive Bayes Accuracy: 0.766478125\n",
            "\n",
            "BernoulliNB Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.75      0.76    159494\n",
            "           1       0.76      0.78      0.77    160506\n",
            "\n",
            "    accuracy                           0.77    320000\n",
            "   macro avg       0.77      0.77      0.77    320000\n",
            "weighted avg       0.77      0.77      0.77    320000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "bnb_pred = bnb.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Bernoulli Naive Bayes Accuracy:\", accuracy_score(y_test, bnb_pred))\n",
        "print(\"\\nBernoulliNB Classification Report:\\n\", classification_report(y_test, bnb_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02iiNydfWCI9"
      },
      "source": [
        "Step 9: Train Support Vector Machine (SVM) model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZHkTXPdWE52",
        "outputId": "78f7a487-8dba-4259-bce9-8a4e290f3782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Accuracy: 0.79528125\n",
            "\n",
            "SVM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.78      0.79    159494\n",
            "           1       0.79      0.81      0.80    160506\n",
            "\n",
            "    accuracy                           0.80    320000\n",
            "   macro avg       0.80      0.80      0.80    320000\n",
            "weighted avg       0.80      0.80      0.80    320000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "svm = LinearSVC(max_iter=1000)\n",
        "svm.fit(X_train_tfidf, y_train)\n",
        "\n",
        "svm_pred = svm.predict(X_test_tfidf)\n",
        "\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
        "print(\"\\nSVM Classification Report:\\n\", classification_report(y_test, svm_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWCYtUNoWgWl"
      },
      "source": [
        "Step 10: Train Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8m1bRfYWo8u",
        "outputId": "f5b1cdaa-6a75-42c5-dd69-a8f6fd003528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Accuracy: 0.79539375\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.78      0.79    159494\n",
            "           1       0.79      0.81      0.80    160506\n",
            "\n",
            "    accuracy                           0.80    320000\n",
            "   macro avg       0.80      0.80      0.80    320000\n",
            "weighted avg       0.80      0.80      0.80    320000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "logreg = LogisticRegression(max_iter=100)\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "\n",
        "logreg_pred = logreg.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logreg_pred))\n",
        "print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, logreg_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWNI7fFDWwZd"
      },
      "source": [
        "Step 11: Make Predictions on sample Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZvh5yprW0F_",
        "outputId": "504f7e0f-bc16-4f86-e893-7c6129fb71ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample Predictions:\n",
            "BernoulliNB: [1 0 1]\n",
            "SVM: [1 0 1]\n",
            "Logistic Regression: [1 0 1]\n"
          ]
        }
      ],
      "source": [
        "sample_tweets = [\"I love this!\", \"I hate that\", \"It was okay, not great.\"]\n",
        "sample_vec = vectorizer.transform(sample_tweets)\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(\"BernoulliNB:\", bnb.predict(sample_vec))\n",
        "print(\"SVM:\", svm.predict(sample_vec))\n",
        "print(\"Logistic Regression:\", logreg.predict(sample_vec))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}